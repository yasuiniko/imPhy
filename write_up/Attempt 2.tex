
\documentclass[12pt]{amsart}
\usepackage{geometry} % see geometry.pdf on how to lay out the page. There's lots.
\geometry{a4paper} % or letter or a5paper or ... etc
% \geometry{landscape} % rotated page geometry

% See the ``Article customise'' template for come common customisations

\title[Missing Leafs]{Reconstructing Phylogenetic Trees with Missing Leafs using Consensus Optimization}
\author{}
\date{} % delete this line to display the current date

%%% BEGIN DOCUMENT
\begin{document}

\maketitle
%\tableofcontents

\section{Introduction}

The problem statement is as follows. Let $S$ be a series of species, $S=1, \dots, |S|$ each with several individuals $N_S=1, \dots, n_s$. For every individual we are provided with a phylogenetic tree: we define $i,j$ to be the indices of the leafs of the tree. Assuming that leafs are missing in some of the trees, we would like to infer the missing distances so as to use them in our analysis. In the literature it is common for missing leafs to be disregarded in any and every tree. Another approach would be to estimate the missing distances by calculating the average of all distances in trees where the missing leafs are present. 

Consensus optimization is an approach to tackling large-scale, non-decomposable mathematical programs. The main idea is that instead of solving one very big and convoluted problem, a decomposition scheme is devised and then $k$ subproblems (of smaller size) are solved. However, seeing as the $k$ subproblems need to agree on exactly one optimal solution (or, reach consensus), the initial solution obtained by the subproblems is bound to be inconsistent. Hence, an iterative ``negotiation" is initiated by the subproblems; in our case this happens in the form of an Augmented Lagrangian term in each objective function. 

\section{Approach}

First, let us define the following parameters and variables. We say that $s, t\in S$ correspond to different species $s$ and $t$, respectively. Furthermore, we say that $m, n\in N_s$ are two individuals ($m$ and $n$) belonging to species $s\in S$. Last, with $i,j$ we refer to the leafs of a phylogenetic tree. Leafs that are both present result in a readily available distance $d_{ijn}$ for some individual $n$. By $x_{ijn}$ we refer to the (missing or present) distance variable between leafs $i$ and $j$ in individual $n$. Hence, the overall problem we are trying to solve is, in optimization form: 

\begin{align}
	QP: \min &~~ \sum\limits_{s\in S}\sum\limits_{t\in S: s\neq t} \sum\limits_{n\in N_s} \sum\limits_{m\in N_t} \sum\limits_{i<j} (x_{ijn}-x_{ijm})^2 \\
	s.t. &~~ x_{ijn} = d_{ijn}, & \forall \textit{ available } d_{ijn}, \forall i,j,n\in N_s, \forall s\in S \\
	&~~ 0\leq x_{ijn}\leq \overline{x}, &\forall i,j,n, \\
	& (x_{ijn}-x_{ijm})^2 \leq \epsilon, &\forall s\in S, \forall n,m\in N_s. 
\end{align}

The objective function captures the (Euclidean) distance between any two individuals belonging to different species and aims to minimize it. Then, the constraints are in order setting the distances of the known (present leafs), ensuring nonnegativity and maximum value of all unknown distances, and, last but not least, ensuring that any distance between leafs of individuals belonging to the same species $s$ can never exceed a threshold value $\epsilon$. 

As this problem can grow to be very large, we propose a decomposition scheme as follows. For each species $s\in S$, we define the following subproblem: 

\begin{align}
	SP_s: \min &~~ \sum\limits_{t\in S: s\neq t} \sum\limits_{n\in N_s} \sum\limits_{m\in N_t} \sum\limits_{i<j} (x_{ijn}-x_{ijm})^2 \\
	s.t. &~~ x_{ijn} = d_{ijn}, & \forall \textit{ available } d_{ijn}, \forall i,j,n\in N_s, \forall s\in S \\
	&~~ 0\leq x_{ijn}\leq \overline{x}, &\forall i,j,n, \\
	&~~ (x_{ijn}-x_{ijm})^2 \leq \epsilon, & \forall i,j, n,m\in N_s. 
\end{align}

Note that the above subproblem only incorporates information that pertain to species $s$ in the original problem ($QP$). The problem now lies with the fact that all $|S|$ subproblems need to reach consensus for the common subvectors of $x$ that they contain. To do that, we create $|S|$ copies of each variable and obtain the formulation: 

\begin{align}
	SP_s: \min &~~ \sum\limits_{t\in S: s\neq t} \sum\limits_{n\in N_s} \sum\limits_{m\in N_t} \sum\limits_{i<j} (x^{(s)}_{ijn}-x^{(s)}_{ijm})^2 \\
	s.t. &~~ x^{(s)}_{ijn} = d_{ijn}, & \forall \textit{ available } d_{ijn}, \forall i,j,n\in N_s, \forall s\in S \\
	&~~ 0\leq x^{(s)}_{ijn}\leq \overline{x}, &\forall i,j,n, \\
	&~~ (x^{(s)}_{ijn}-x^{(s)}_{ijm})^2 \leq \epsilon, & \forall i,j, n,m\in N_s, \\
	&~~ x^{(s)}_{ijn} = x^{(t)}_{ijn}, & \forall i, j, n, \forall s\neq t.
\end{align}

The last constraint equates all different copies of the variables. As easily seen, this will lead often to infeasible solutions. Hence, we dualize the coupling constraint and add it to the objective function as an Augmented Lagrangian term in the objective function. In the next formulation, $\rho$ can be viewed as the quadratic penalty factor and $\lambda$ as the respective vector of the Lagrange multipliers. 

\begin{align}
	SP_s: \min &~~ \sum\limits_{t\in S: s\neq t} \sum\limits_{n\in N_s} \sum\limits_{m\in N_t} \sum\limits_{i<j} (x^{(s)}_{ijn}-x^{(s)}_{ijm})^2 \\
	\nonumber ~~&+\sum\limits_{t_1\neq s}\lambda_{st_1} \sum\limits_{t\in S}\sum\limits_{n\in N_t} (x^{(s)}_{ijn} - x^{(t)} _{ijn}) + \frac{\rho^2}{2}\sum\limits_{t_1\neq s}\lambda_{st_1} \sum\limits_{t\in S}\sum\limits_{n\in N_t} (x^{(s)}_{ijn} - x^{(t)}_{ijn})^2 \\
	s.t. &~~ x^{(s)}_{ijn} = d_{ijn}, ~~~ \forall \textit{ available } d_{ijn}, \forall i,j,n\in N_s, \forall s\in S \\
	&~~ 0\leq x^{(s)}_{ijn}\leq \overline{x}, ~~~\forall i,j,n, \\
	\label{EpsilonConstraint}
	&~~ (x^{(s)}_{ijn}-x^{(s)}_{ijm})^2 \leq \epsilon, ~~~ \forall i,j, n,m\in N_s.
\end{align}

Updating the Lagrange multipliers can be performed using \eqref{updateLagrange}. 

\begin{align}
	\label{updateLagrange}
	\lambda_{st} \leftarrow \lambda_{st} + \rho(x^{(s)}-x^{(t)})
\end{align}

Last, the penalty factor can be updated using a non-decreasing function of the iteration. As an example, $\rho\leftarrow\rho\cdot\alpha$ or $\rho\leftarrow\rho^\alpha$ can be used. 

\section{Linear Regression}

The above optimization problem behaves like a \emph{linear regression} model. Since the objective function that is minimized captures the ``distance" (Euclidean or 2-norm) between the leaf distances between a species individual ($n\in N_s$) and other species individuals ($m\in N_t, t\neq s$), the result for a pair of leafs $(i, j)$ is a line of the form:

$$d_{ijn} = \sum\limits_{t\in S\setminus\{s\}}\sum\limits_{m\in N_t} \alpha_m d_{ijm}.$$

Seeing as no individuals of the same species are present in the objective function, they are also not present in the linear regression. However, seeing as there exists a constraint that ensures individuals within the same species are not too far apart (see, for example, constraint \eqref{EpsilonConstraint}), then if the linear regression results in a pair distance that does not satisfy that requirement, the pair distance is updated and modified. The parameters $\alpha_m$ for every individual ends up being equal to $\frac{1}{\sum\limits_{t\in S\setminus\{s\}} |N_t|}$ (hence, producing a weighted average based on the number of individuals in each species). We portray this with an example. 

\paragraph{Example.} Consider the three species shown in Tables \ref{species1}, \ref{species2}, and \ref{species3}, where pair distances that are equal to -1 imply that either one or both leafs are missing. 

\begin{table}[h]
\caption{Species 1 distances. \label{species1}}
\begin{tabular}{l|cccccc}
Tree 1 & 6 &  8 & 7 & 3 &  4 & 3 \\
Tree 2 & -1 & -1 & -1 &  4 & 4 &  4 \\
Tree 3 & 5 &  -1&  7&  -1&  -1&  3 \\
Tree 4 & 6 & 7 & -1 & 5 & -1 & -1 
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Species 2 distances. \label{species2}}
\begin{tabular}{l|cccccc}
Tree 1&6& 3& 2& 5& 1& 4\\
Tree 2&-1& -1& -1& 5& 2& 3\\
Tree 3&9& -1& 8& -1& -1& 2\\
Tree 4&5& 3& -1& 5& -1& -1\\
Tree 5&5& -1& -1& -1& -1& -1 
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Species 3 distances. \label{species3}}
\begin{tabular}{l|cccccc}
Tree 1&3& 3& 7& 5& 6& 7\\
Tree 2&-1& -1& -1& 3& 6& 6\\
Tree 3&5& -1& 6& -1& -1& 8
\end{tabular}
\end{table}

Solving the model when setting $\epsilon=\textit{max distance}$, we obtain the results in Tables \ref{norm1}, \ref{norm2}, and \ref{norm3}, whereas if we let $\epsilon=M$ (where $M$ is big enough), we have the Tables \ref{bigM1}, \ref{bigM2}, and \ref{bigM3}. 

\begin{table}[h]
\caption{Species 1 solution. \label{norm1}}
\begin{tabular}{l|cccccc}
Tree 1 & 6 &  8 & 7 & 3 &  4 & 3 \\
Tree 2 & \textbf{5.38}&	\textbf{6}&	\textbf{6.08} &  4 & 4 &  4 \\
Tree 3 & 5 &  \textbf{6}&  7&  \textbf{4.46}&  \textbf{4.1}&  3 \\
Tree 4 & 6 & 7 & \textbf{6.08} & 5 & \textbf{4.1} & \textbf{5} 
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Species 2 solution. \label{norm2}}
\begin{tabular}{l|cccccc}
Tree 1&6& 3& 2& 5& 1& 4\\
Tree 2&\textbf{5.05}& \textbf{5.71}& \textbf{6.48}& 5& 2& 3\\
Tree 3&9& \textbf{5.71}& 8& \textbf{4.12}& \textbf{4.6}& 2\\
Tree 4&5& 3& \textbf{6.48}& 5&  \textbf{4.6}& \textbf{5.14}\\
Tree 5&5& \textbf{5.71}& \textbf{6.48}& \textbf{4.12}& \textbf{4.6}& \textbf{5.14} 
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Species 3 solution. \label{norm3}}
\begin{tabular}{l|cccccc}
Tree 1&3& 3& 7& 5& 6& 7\\
Tree 2&\textbf{5}& \textbf{5}& \textbf{6.18}& 3& 6& 6\\
Tree 3&5& \textbf{5}& 6& \textbf{4.41}& \textbf{4}& 8
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Species 1 solution with big M. \label{bigM1}}
\begin{tabular}{l|cccccc}
Tree 1 & 6 &  8 & 7 & 3 &  4 & 3 \\
Tree 2 & \textbf{5.51}&	\textbf{4.36}&	\textbf{6.07} &  4 & 4 &  4 \\
Tree 3 & 5 &  \textbf{4.36}&  7&  \textbf{4.46}&  \textbf{4.02}&  3 \\
Tree 4 & 6 & 7 & \textbf{6.07} & 5 & \textbf{4.02} & \textbf{5.05} 
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Species 2 solution with big M. \label{bigM2}}
\begin{tabular}{l|cccccc}
Tree 1&6& 3& 2& 5& 1& 4\\
Tree 2&\textbf{5.19}& \textbf{5.26}& \textbf{6.47}& 5& 2& 3\\
Tree 3&9& \textbf{5.26}& 8& \textbf{4.12}& \textbf{4.52}& 2\\
Tree 4&5& 3& \textbf{6.47}& 5&  \textbf{4.52}& \textbf{5.15}\\
Tree 5&5& \textbf{5.26}& \textbf{6.47}& \textbf{4.12}& \textbf{4.52}& \textbf{5.15} 
\end{tabular}
\end{table}

\begin{table}[h]
\caption{Species 3 solution with big M. \label{bigM3}}
\begin{tabular}{l|cccccc}
Tree 1&3& 3& 7& 5& 6& 7\\
Tree 2&\textbf{5.86}& \textbf{5.06}& \textbf{6.17}& 3& 6& 6\\
Tree 3&5& \textbf{5.06}& 6& \textbf{4.41}& \textbf{3.62}& 8
\end{tabular}
\end{table}

Let us point out attention first to Species 1, Tree 2, where the biggest difference is observed. In the 2nd pair of leafs, the results with constraint \eqref{EpsilonConstraint} show as optimal distance the value 6, whereas once the constraint is effectively lifted, the optimal distance becomes 4.36. Let us see where this value comes from: if we see the optimal distances when letting $\epsilon$ be big enough is the average of the other two species distances for the same pair of leafs. That is, (3+5.26+5.26+3+5.26+3+5.06+5.06)/8=34.9/8=4.36. Hence, under the absence of \eqref{EpsilonConstraint} the optimization problem is akin to calculating an average. On the other hand, under the presence of \eqref{EpsilonConstraint}, the optimization problem can no longer select 4.36 as optimal, since it now is \emph{infeasible}. Hence, it changes the distance value to become feasible by setting it equal to a value that is within the maximum distance present in that particular species (in this example, the maximum distance between pair distances for Species 1 is 2, coming from the 4th pairwise distance of Trees 1 and 4). 

A small observation: changing the distance between the 4th pair of leafs for Tree 4 of Species 1 from 5 to 4, results in changing the optimal value for the above distance from 6 to 7, as now the maximum ``allowed" distance within the first species is 1.  

\end{document}